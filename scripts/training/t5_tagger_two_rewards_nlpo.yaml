tokenizer:
  model_name: /home/raffoni/DearWatson/best_checkpoint/t5-sl
  padding_side: right
  truncation_side: right
  pad_token_as_eos_token: True

shared_summarization_model:
  load_path: "/home/italiani/dear-watson/DearWatson/bart_large_sl/best_model.pth"
  max_new_tokens: 100
  # num_beams: 5
  device: "cuda:1"
  base_model: "facebook/bart-large"
  load_from_state_dict: True

reward_fn:
  id: summarizer_rouge
  args:
    alpha: 0.5

datapool:
  id: annotated_samsum
  args:
    clean_dataset_path: "/datasets/Dialogue/SAMsum/clean"
    annotated_dataset_path: "/datasets/Dialogue/SAMsum/sl"
    target_path: "/datasets/Dialogue/SAMsum/targets"

env:
  n_envs: 4
  args:
    max_prompt_length: 256
    max_episode_length: 256
    terminate_on_eos: True
    prompt_truncation_side: "right"
    context_start_token: 0

alg:
  id: nlpo
  args:
    n_steps: 256
    batch_size: 4
    verbose: 0
    learning_rate: 0.0000005
    n_epochs: 5
    ent_coef: 0.02
    gamma: 0.998
    gae_lambda: 0.95
    vf_coef: 0.2
    clip_range: 0.2
  kl_div:
    coeff: 0.3
    target_kl: 0.1
  policy:
    id: maskable_seq2seq_lm_actor_critic_policy
    args:
      model_name: /home/raffoni/DearWatson/best_checkpoint/t5-sl
      apply_model_parallel: True
      prompt_truncation_side: "right"
      top_mask: 0.9
      mask_type: "learned_top_p"
      target_update_iterations: 30
      generation_kwargs:
        do_sample: True
        top_k: 30
        min_length: 40
        max_new_tokens: 512
      # freeze_encoder: False

train_evaluation:
  eval_batch_size: 16
  n_iters: 25
  eval_every: 1
  save_every: 5
  metrics:
    - id: augmented_summarization_rouge
      args:
        batch_size: 16
    - id: tagging_accuracy
  generation_kwargs:
    do_sample: True
    top_k: 0
    temperature: 0.9
    min_length: 40
    max_new_tokens: 512
